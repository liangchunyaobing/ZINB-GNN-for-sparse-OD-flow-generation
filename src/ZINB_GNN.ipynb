{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This code is partly adapted from: https://github.com/ZhuangDingyi/STZINB"
      ],
      "metadata": {
        "id": "wyrWr9OMrPQ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q1oCRYsdJWvY"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "from scipy.linalg import eigvalsh\n",
        "from scipy.linalg import fractional_matrix_power\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pickle as pkl\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "import time\n",
        "from scipy.stats import nbinom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b76a831-2ce6-464b-8563-868aea9d9558",
        "scrolled": true,
        "id": "ZYaBhIJvzlDk"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJbDSzIMKF0P"
      },
      "source": [
        "# Spatial Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Uy0QF8LwAoii"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class InterGAT(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network block that applies attention mechanism to sampled locations (only the attention).\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, alpha=0.2, threshold=0.0, concat = True):\n",
        "        \"\"\"\n",
        "        :param in_channels: Number of time step.\n",
        "        :param alpha: alpha for leaky Relu.\n",
        "        :param threshold: threshold for graph connection\n",
        "        :param concat: whether concat features\n",
        "        :It should be noted that the input layer should use linear activation\n",
        "        \"\"\"\n",
        "        super(InterGAT, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.threshold = threshold\n",
        "        self.concat = concat\n",
        "        self.in_channels = in_channels\n",
        "#         self.a = nn.Parameter(torch.zeros(size=(2*in_channels + 1, 1)))\n",
        "        self.attn1 = nn.Linear(2*in_channels, 16)\n",
        "        self.attn2 = nn.Linear(16, 1)\n",
        "#         nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, input_target, input_neigh, adj):\n",
        "        \"\"\"input_target [batch_size, n_feat]\"\"\"\n",
        "        \"\"\"input_neigh [batch_size, n_neigh, n_feat]\"\"\"\n",
        "        \"\"\"adj [batch_size, n_neigh]\"\"\"\n",
        "\n",
        "        B = input_neigh.size()[0]\n",
        "        N = input_neigh.size()[1]\n",
        "        # h_query = h[:, :, :-2]\n",
        "\n",
        "        # a_input = torch.cat([h.repeat(1, 1, N).view(B, N * N, self.in_channels), h.repeat(1, N, 1)], dim=2).view(B, N, N, 2 * self.in_channels)\n",
        "        a_input = torch.cat([input_target.unsqueeze(1).repeat(1, N, 1), input_neigh], -1) # batch_size, n_neigh, n_feat\n",
        "        # a_input = a_input.unsqueeze(1).unsqueeze(1).repeat(1, 2, 24, 1, 1, 1)\n",
        "        # a_input = torch.cat([a_input, tmp_emb.unsqueeze(-2).unsqueeze(-2).repeat(1, 1, 1, N, N, 1)], -1)\n",
        "        e = self.leakyrelu(self.attn2(self.relu(self.attn1(a_input)))).squeeze(-1) # batch_size, n_neigh\n",
        "#         e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(3))\n",
        "        zero_vec = -9e15*torch.ones_like(e).to(device)\n",
        "        # print(\"adj\", adj.device, \"e\", e.device, \"zero_vec\", zero_vec.device)\n",
        "        attention = torch.where(adj > self.threshold, e, zero_vec) #>threshold for attention connection\n",
        "        attention = F.softmax(attention, dim=-1) # batch_size, n_neigh\n",
        "        h_prime = torch.matmul(attention.unsqueeze(1), input_neigh).squeeze(1) # batch_size, 1, n_feat\n",
        "        # batch_size, n_feat\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime\n",
        "\n",
        "class InterMGCN(nn.Module):\n",
        "    def __init__(self, c_in, c_out, graph_conv_act_func, enable_bias=True):\n",
        "        super(InterMGCN, self).__init__()\n",
        "        self.c_in = c_in\n",
        "        self.c_out = c_out\n",
        "        self.enable_bias = enable_bias\n",
        "        self.graph_conv_act_func = graph_conv_act_func\n",
        "        self.relu = nn.ReLU()\n",
        "        self.weight = nn.Parameter(torch.Tensor(c_in, c_out).float().to(device))\n",
        "        if enable_bias == True:\n",
        "            self.bias = nn.Parameter(torch.Tensor(c_out).float().to(device))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        # For Sigmoid or Tanh\n",
        "        if self.graph_conv_act_func == 'sigmoid' or self.graph_conv_act_func == 'tanh':\n",
        "            init.xavier_uniform_(tensor=self.weight, gain=init.calculate_gain(self.graph_conv_act_func))\n",
        "        # For ReLU, LeakyReLU, or ELU\n",
        "        elif self.graph_conv_act_func == 'relu' or self.graph_conv_act_func == 'leaky_relu' or self.graph_conv_act_func == 'elu':\n",
        "            init.kaiming_uniform_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, input_target, input_neigh, adj):\n",
        "        \"\"\"input_target [batch_size, n_feat]\"\"\"\n",
        "        \"\"\"input_neigh [batch_size, n_node, n_feat]\"\"\"\n",
        "        \"\"\"adj [batch_size, n_node]\"\"\"\n",
        "        batch_size, n_vertex, c_in = input_neigh.shape # x=[batch_size, c_in, n_node] # the size of\n",
        "        # x_first_target = input_target @ self.weight\n",
        "        x_first_mul = input_neigh.reshape(-1, c_in) @ self.weight\n",
        "        x_first_mul = input_neigh.view(batch_size, n_vertex, -1)\n",
        "        x_second_mul = torch.matmul(adj.unsqueeze(1), x_first_mul).squeeze(1) # batch_size, c_out\n",
        "        if self.bias is not None:\n",
        "            x_gcnconv = x_second_mul + self.bias\n",
        "        else:\n",
        "            x_gcnconv = x_second_mul\n",
        "        return self.relu(x_gcnconv)\n",
        "\n",
        "class InterGraphConvLayer(nn.Module):\n",
        "    def __init__(self, Ks, c_in, c_out, graph_conv_type, graph_conv_act_func):\n",
        "        super(InterGraphConvLayer, self).__init__()\n",
        "        self.Ks = Ks\n",
        "        # self.c_in = c_in\n",
        "        # self.c_out = c_out\n",
        "        # self.align = Align(c_in, c_out)\n",
        "        self.graph_conv_type = graph_conv_type\n",
        "        self.graph_conv_act_func = graph_conv_act_func\n",
        "        self.enable_bias = True\n",
        "        if self.graph_conv_type == \"gat\":\n",
        "            self.align = nn.Linear(c_in, c_out)\n",
        "            self.gcnconv = InterGAT(c_out)\n",
        "        else:\n",
        "            self.gcnconv = InterMGCN(c_in, c_out, graph_conv_act_func)\n",
        "\n",
        "    def forward(self, x_target, x_neigh, graph_conv_matrix=None):\n",
        "        # x [batch_size, ..., n_feat]\n",
        "        # graph_conv_matrix [batch_size, ..., n_node, n_node]\n",
        "        if self.graph_conv_type == \"gat\":\n",
        "            x_target = self.align(x_target)\n",
        "            x_neigh = self.align(x_neigh)\n",
        "        x_gc_with_rc = self.gcnconv(x_target, x_neigh, graph_conv_matrix)\n",
        "        return x_gc_with_rc # [batch_size, c_out, n_vertex]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKNljsgbyM_0"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QPC48v1pyPkO"
      },
      "outputs": [],
      "source": [
        "def nb_nll_loss(y,n,p,y_mask=None, weight=True):\n",
        "    \"\"\"\n",
        "    y: true values\n",
        "    y_mask: whether missing mask is given\n",
        "    \"\"\"\n",
        "    if y_mask is not None:\n",
        "      y = y[y_mask > 0]\n",
        "      n = n[y_mask > 0]\n",
        "      p = p[y_mask > 0]\n",
        "    nll = torch.lgamma(n) + torch.lgamma(y+1) - torch.lgamma(n+y) - n*torch.log(p) - y*torch.log(1-p)\n",
        "    return torch.sum(nll)\n",
        "\n",
        "def nb_zeroinflated_nll_loss(y,n,p,pi,y_mask=None, weight=0.):\n",
        "    \"\"\"\n",
        "    y: true values\n",
        "    y_mask: whether missing mask is given\n",
        "    https://stats.idre.ucla.edu/r/dae/zinb/\n",
        "    \"\"\"\n",
        "    if y_mask is not None:\n",
        "      y = y[y_mask > 0]\n",
        "      n = n[y_mask > 0]\n",
        "      p = p[y_mask > 0]\n",
        "      pi = pi[y_mask > 0]\n",
        "\n",
        "    idx_yeq0 = y==0\n",
        "    idx_yg0  = y>0\n",
        "\n",
        "    n_yeq0 = n[idx_yeq0]\n",
        "    p_yeq0 = p[idx_yeq0]\n",
        "    pi_yeq0 = pi[idx_yeq0]\n",
        "    yeq0 = y[idx_yeq0]\n",
        "\n",
        "    n_yg0 = n[idx_yg0]\n",
        "    p_yg0 = p[idx_yg0]\n",
        "    pi_yg0 = pi[idx_yg0]\n",
        "    yg0 = y[idx_yg0]\n",
        "\n",
        "    #L_yeq0 = torch.log(pi_yeq0) + (1-pi_yeq0)*torch.pow(p_yeq0,n_yeq0)\n",
        "    #L_yg0  = torch.log(pi_yg0) + torch.lgamma(n_yg0+yg0) - torch.lgamma(yg0+1) - torch.lgamma(n_yg0) + n_yg0*torch.log(p_yg0) + yg0*torch.log(1-p_yg0)\n",
        "    L_yeq0 = torch.log(pi_yeq0) + torch.log((1-pi_yeq0)*torch.pow(p_yeq0,n_yeq0))\n",
        "    L_yg0  = torch.log(1-pi_yg0) + torch.lgamma(n_yg0+yg0) - torch.lgamma(yg0+1) - torch.lgamma(n_yg0) + n_yg0*torch.log(p_yg0) + yg0*torch.log(1-p_yg0)\n",
        "    #print('nll',torch.mean(L_yeq0),torch.mean(L_yg0),torch.mean(torch.log(pi_yeq0)),torch.mean(torch.log(pi_yg0)))\n",
        "    if weight > 0:\n",
        "        # od_weight = torch.pow(yg0, weight)\n",
        "        # od_weight[yg0 < 1] = 0\n",
        "    #     L_yg0 = L_yg0 * torch.exp(yg0 / weight)\n",
        "    # if focal is True:\n",
        "        L_yeq0 = (1-torch.exp(L_yeq0)) ** weight * L_yeq0\n",
        "        L_yg0 = (1-torch.exp(L_yg0)) ** weight * L_yg0\n",
        "    return -torch.sum(L_yeq0)-torch.sum(L_yg0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY3AIYPDzM3n"
      },
      "source": [
        "# Prediction Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2lrecZPSzTJs"
      },
      "outputs": [],
      "source": [
        "class FNN_NBNorm(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout, setting=\"daily_cnt\", prob=\"ZINB\"):\n",
        "        super(FNN_NBNorm, self).__init__()\n",
        "        self.setting = setting\n",
        "        self.prob = prob\n",
        "        self.n_fc =nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(128, output_dim)\n",
        "          )\n",
        "        self.p_fc = nn.Sequential(\n",
        "          nn.Linear(input_dim, 256),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(p=dropout),\n",
        "          nn.Linear(256, 128),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(p=dropout),\n",
        "          nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "          \"\"\"ori_dist_feat [batch_size, n_neigh, n_feat]\"\"\"\n",
        "          \"\"\"ori_dist_adj [batch_size, n_neigh]\"\"\"\n",
        "          \"\"\"des_dist_feat [batch_size, n_global_sta, n_neigh, n_feat]\"\"\"\n",
        "          \"\"\"des_dist_adj [batch_size, n_global_sta, n_neigh]\"\"\"\n",
        "          \"\"\"od_feat [batch_size, n_global_sta, n_feat]\"\"\"\n",
        "          \"\"\"od_dist_feat [batch_size, n_global_sta, (n_neigh+1)*(n_neigh+1)-1, n_feat]\"\"\"\n",
        "          \"\"\"od_dist_adj [batch_size, n_global_sta, (n_neigh+1)*(n_neigh+1)-1]\"\"\"\n",
        "          \"\"\"x_month [batch_size]\"\"\"\n",
        "          n = self.n_fc(x)\n",
        "          p = self.p_fc(x)\n",
        "          n = F.softplus(n) # Some parameters can be tuned here\n",
        "          p = F.sigmoid(p)\n",
        "          return n.squeeze(-1), p.squeeze(-1)\n",
        "\n",
        "class FNN_NBNorm_ZeroInflated(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout, setting=\"daily_cnt\"):\n",
        "        super(FNN_NBNorm_ZeroInflated, self).__init__()\n",
        "        self.setting = setting\n",
        "        self.n_fc =nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(128, output_dim)\n",
        "          )\n",
        "        self.p_fc = nn.Sequential(\n",
        "          nn.Linear(input_dim, 256),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(p=dropout),\n",
        "          nn.Linear(256, 128),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(p=dropout),\n",
        "          nn.Linear(128, output_dim)\n",
        "        )\n",
        "        self.pi_fc = nn.Sequential(\n",
        "          nn.Linear(input_dim, 256),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(p=dropout),\n",
        "          nn.Linear(256, 128),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(p=dropout),\n",
        "          nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, n_max_station, _ = x.shape\n",
        "        n = self.n_fc(x)\n",
        "        p = self.p_fc(x)\n",
        "        pi = self.pi_fc(x)\n",
        "        n = F.softplus(n) # Some parameters can be tuned here\n",
        "        p = F.sigmoid(p)\n",
        "        pi = F.sigmoid(pi)\n",
        "        if \"daily\" in self.setting:\n",
        "          return n.squeeze(-1), p.squeeze(-1), pi.squeeze(-1)\n",
        "        return n.reshape(batch_size, n_max_station, 2, 3), p.reshape(batch_size, n_max_station, 2, 3), pi.reshape(batch_size, n_max_station, 2, 3)\n",
        "\n",
        "class FNN_Prediction(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout, setting=\"daily_cnt\"):\n",
        "        super(FNN_Prediction, self).__init__()\n",
        "        self.setting = setting\n",
        "        self.out_fc =nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(128, output_dim)\n",
        "          )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, n_max_station, _ = x.shape\n",
        "        y = self.out_fc(x)\n",
        "        if \"daily\" in self.setting:\n",
        "          return y.squeeze(-1)\n",
        "        return y.reshape(batch_size, n_max_station, 2, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZT7T75dKtIt"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FKfkn8PTHZZS"
      },
      "outputs": [],
      "source": [
        "class Graph_NBNorm_ZeroInflated(nn.Module):\n",
        "    def __init__(self, dim_station, dim_interact, n_month_embed=12, dropout=0.5, setting=\"daily_density\",\n",
        "                 dist=\"zinb\", graph_conv_type=\"gat\", od_graph=True, node_graph=True, builtEnv=True,\n",
        "                 n_gcn=8, n_od_gcn=8):\n",
        "        super(Graph_NBNorm_ZeroInflated,self).__init__()\n",
        "        \"\"\"setting: daily_density, hour_density\"\"\"\n",
        "        \"\"\"dist: zinb, zip, na\"\"\"\n",
        "        \"\"\"graph_conv_type: gat, gcn\"\"\"\n",
        "        \"\"\"od_graph: true & false\"\"\"\n",
        "        \"\"\"node_graph: true & false\"\"\"\n",
        "        self.month_embedding = nn.Embedding(12, n_month_embed)\n",
        "        self.setting = setting\n",
        "        self.dist = dist\n",
        "        self.graph_conv_type = graph_conv_type\n",
        "        self.od_graph = od_graph\n",
        "        self.node_graph = node_graph\n",
        "        self.builtEnv = builtEnv\n",
        "        self.graph_conv_act_func = \"relu\"\n",
        "        \"\"\"feature encoding layer\"\"\"\n",
        "        input_dim = dim_station*2+dim_interact+n_month_embed\n",
        "        if self.node_graph:\n",
        "          self.ori_gcn_dist = InterGraphConvLayer(1, dim_station, n_gcn, self.graph_conv_type, self.graph_conv_act_func)\n",
        "          input_dim += n_gcn\n",
        "          if self.builtEnv:\n",
        "            self.ori_gcn_builtEnv = InterGraphConvLayer(1, dim_station, n_gcn, self.graph_conv_type, self.graph_conv_act_func)\n",
        "            input_dim += n_gcn\n",
        "          input_dim += n_gcn\n",
        "\n",
        "        if self.od_graph:\n",
        "          self.od_gcn_dist = InterGraphConvLayer(1, dim_station * 2 + dim_interact, n_od_gcn, self.graph_conv_type, self.graph_conv_act_func)\n",
        "          input_dim += n_od_gcn\n",
        "          if self.builtEnv:\n",
        "            self.od_gcn_builtEnv = InterGraphConvLayer(1, dim_station * 2 + dim_interact, n_od_gcn, self.graph_conv_type, self.graph_conv_act_func)\n",
        "            input_dim += n_od_gcn\n",
        "        if self.setting == \"daily_cnt\":\n",
        "            input_dim += 1\n",
        "        self.input_dim = input_dim\n",
        "        \"\"\"prediction layer\"\"\"\n",
        "        if \"daily\" in self.setting:\n",
        "          output_dim = 1\n",
        "        elif \"hour\" in self.setting:\n",
        "          output_dim = 6\n",
        "        self.output_dim = output_dim\n",
        "        if self.dist == \"zinb\":\n",
        "          self.pred_fc = FNN_NBNorm_ZeroInflated(self.input_dim, self.output_dim, dropout=dropout, setting=setting)\n",
        "        elif self.dist == \"nb\":\n",
        "          self.pred_fc = FNN_NBNorm(self.input_dim, self.output_dim, dropout=dropout, setting=setting)\n",
        "        else:\n",
        "          self.pred_fc = FNN_Prediction(self.input_dim, self.output_dim, dropout=dropout, setting=setting)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def process_od_features(self, ori_feat, ori_neigh_feat, des_feat, des_neigh_feat, od_feat, od_neigh_feat):\n",
        "        batch_size, n_global_station, n_neigh, n_station_feat = des_neigh_feat.shape\n",
        "        x_od_ori_feat = torch.cat([ori_feat.unsqueeze(1), ori_neigh_feat], 1).unsqueeze(1).repeat(1, n_global_station, 1, 1)\n",
        "        x_od_des_feat = torch.cat([des_feat.unsqueeze(2), des_neigh_feat], 2)\n",
        "        x_od_ori_feat = x_od_ori_feat.unsqueeze(-2).repeat(1, 1, 1, n_neigh+1, 1)\n",
        "        x_od_des_feat = x_od_des_feat.unsqueeze(-3).repeat(1, 1, n_neigh+1, 1, 1)\n",
        "        x_od_ori_des_feat = torch.cat([x_od_ori_feat, x_od_des_feat], -1).reshape(batch_size, n_global_station, (n_neigh+1)*(n_neigh+1), -1)\n",
        "        if self.setting == \"daily_density\":\n",
        "            x_od_target = torch.cat([x_od_ori_des_feat[:, :, 0], od_feat], -1)\n",
        "        else:\n",
        "            x_od_target = torch.cat([x_od_ori_des_feat[:, :, 0], od_feat[:, :, :-1]], -1)\n",
        "        x_od_neigh = torch.cat([x_od_ori_des_feat[:, :, 1:], od_neigh_feat], -1)\n",
        "        return x_od_target, x_od_neigh\n",
        "\n",
        "    def forward(self, ori_feat, ori_dist_feat, ori_dist_adj, ori_builtEnv_feat, ori_builtEnv_adj,\n",
        "                des_feat, des_dist_feat, des_dist_adj, des_builtEnv_feat, des_builtEnv_adj,\n",
        "                od_feat, od_dist_feat, od_dist_adj, od_builtEnv_feat, od_builtEnv_adj,\n",
        "                x_month, des_mask):\n",
        "        \"\"\"ori_dist_feat [batch_size, n_neigh, n_feat]\"\"\"\n",
        "        \"\"\"ori_dist_adj [batch_size, n_neigh]\"\"\"\n",
        "        \"\"\"des_dist_feat [batch_size, n_global_sta, n_neigh, n_feat]\"\"\"\n",
        "        \"\"\"des_dist_adj [batch_size, n_global_sta, n_neigh]\"\"\"\n",
        "        \"\"\"od_feat [batch_size, n_global_sta, n_feat]\"\"\"\n",
        "        \"\"\"od_dist_feat [batch_size, n_global_sta, (n_neigh+1)*(n_neigh+1)-1, n_feat]\"\"\"\n",
        "        \"\"\"od_dist_adj [batch_size, n_global_sta, (n_neigh+1)*(n_neigh+1)-1]\"\"\"\n",
        "        \"\"\"x_month [batch_size]\"\"\"\n",
        "        batch_size, n_global_station, n_neigh, n_station_feat = des_dist_feat.shape\n",
        "        \"\"\"load od features\"\"\"\n",
        "        x_od_target, x_od_dist = self.process_od_features(ori_feat, ori_dist_feat, des_feat, des_dist_feat, od_feat, od_dist_feat)\n",
        "        if self.builtEnv:\n",
        "          _, x_od_builtEnv = self.process_od_features(ori_feat, ori_builtEnv_feat, des_feat, des_builtEnv_feat, od_feat, od_builtEnv_feat)\n",
        "        x_month = self.month_embedding(x_month)\n",
        "        x_od_feat_ls = [x_od_target, x_month.unsqueeze(1).repeat(1, n_global_station, 1)]\n",
        "        \"\"\"origin spatial fature encoding\"\"\"\n",
        "        if self.node_graph:\n",
        "          x_ori_graph_dist = self.ori_gcn_dist(ori_feat, ori_dist_feat, ori_dist_adj) # batch_size, c\n",
        "          x_od_feat_ls.append(x_ori_graph_dist.unsqueeze(1).repeat(1, n_global_station, 1))\n",
        "          if self.builtEnv:\n",
        "            x_ori_graph_builtEnv = self.ori_gcn_builtEnv(ori_feat, ori_dist_feat, ori_dist_adj) # batch_size, c\n",
        "            x_od_feat_ls.append(x_ori_graph_builtEnv.unsqueeze(1).repeat(1, n_global_station, 1))\n",
        "        \"\"\"destination spatial feature encoding\"\"\"\n",
        "        if self.node_graph:\n",
        "          x_des_graph_dist = self.ori_gcn_dist(des_feat.reshape(batch_size * n_global_station, -1),\n",
        "                            des_dist_feat.reshape(batch_size * n_global_station, n_neigh, -1),\n",
        "                            des_dist_adj.reshape(batch_size * n_global_station, n_neigh)).reshape(\\\n",
        "                            batch_size, n_global_station, -1) # batch_size, c\n",
        "          x_od_feat_ls.append(x_des_graph_dist)\n",
        "          if self.builtEnv:\n",
        "            x_des_graph_builtEnv = self.ori_gcn_builtEnv(des_feat.reshape(batch_size * n_global_station, -1),\n",
        "                            des_builtEnv_feat.reshape(batch_size * n_global_station, n_neigh, -1),\n",
        "                            des_builtEnv_adj.reshape(batch_size * n_global_station, n_neigh)).reshape(\\\n",
        "                            batch_size, n_global_station, -1) # batch_size, c\n",
        "            x_od_feat_ls.append(x_des_graph_builtEnv)\n",
        "        \"\"\"od dist-based spatial feature encoding\"\"\"\n",
        "        if self.od_graph:\n",
        "          x_od_graph_dist = self.od_gcn_dist(x_od_target.reshape(batch_size * n_global_station, -1),\n",
        "                           x_od_dist.reshape(batch_size * n_global_station, (n_neigh+1)*(n_neigh+1)-1, -1),\n",
        "                           od_dist_adj.reshape(batch_size * n_global_station, (n_neigh+1)*(n_neigh+1)-1)).reshape(\\\n",
        "                           batch_size, n_global_station, -1) # batch_size, c\n",
        "          x_od_feat_ls.append(x_od_graph_dist)\n",
        "          if self.builtEnv:\n",
        "            x_od_graph_builtEnv = self.od_gcn_builtEnv(x_od_target.reshape(batch_size * n_global_station, -1),\n",
        "                           x_od_builtEnv.reshape(batch_size * n_global_station, (n_neigh+1)*(n_neigh+1)-1, -1),\n",
        "                           od_builtEnv_adj.reshape(batch_size * n_global_station, (n_neigh+1)*(n_neigh+1)-1)).reshape(\\\n",
        "                           batch_size, n_global_station, -1) # batch_size, c\n",
        "            x_od_feat_ls.append(x_od_graph_builtEnv)\n",
        "        if self.setting == \"daily_cnt\":\n",
        "            x_od_feat_ls.append(od_feat[:, :, -1:])\n",
        "        \"\"\"prediction\"\"\"\n",
        "        x_od = torch.cat(x_od_feat_ls, -1)\n",
        "        return self.pred_fc(x_od)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDCLLOfcTxHB"
      },
      "source": [
        "# Data Container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3FV4oIV6VDEd"
      },
      "outputs": [],
      "source": [
        "# data_dir=data_dir, knn_dir = knn_dir\n",
        "class DataInput(object):\n",
        "    def __init__(self, local2global_station_dir, month_od_opendays_dir, month_od_nonzero_dir,\n",
        "                 feat_dir, od_feat_dir, month_feat_dir, month_od_feat_dir,\n",
        "                 manhattan_dir, month_station_opendays_dir,\n",
        "                 dist_knn_dir, dist_adj_dir,\n",
        "                 builtEnv_knn_dir, builtEnv_adj_dir,\n",
        "                 n_train_timestep=50, k=5,\n",
        "                 setting=\"daily_cnt\"):\n",
        "        self.timestep_ls = self.load_timestep_ls()\n",
        "        print(\"done load timestep_ls...\")\n",
        "        self.local2global_station_dir = local2global_station_dir\n",
        "        self.month_od_opendays_dir = month_od_opendays_dir\n",
        "        self.month_od_nonzero_dir = month_od_nonzero_dir\n",
        "        self.feat_dir = feat_dir\n",
        "        self.od_feat_dir = od_feat_dir\n",
        "        self.month_feat_dir = month_feat_dir\n",
        "        self.month_od_feat_dir = month_od_feat_dir\n",
        "        self.manhattan_dir = manhattan_dir\n",
        "        self.month_station_opendays_dir = month_station_opendays_dir\n",
        "        self.dist_knn_dir = dist_knn_dir\n",
        "        self.dist_adj_dir = dist_adj_dir\n",
        "        self.builtEnv_knn_dir = builtEnv_knn_dir\n",
        "        self.builtEnv_adj_dir = builtEnv_adj_dir\n",
        "        self.n_train_timestep = n_train_timestep\n",
        "        self.k = k\n",
        "        self.setting = setting\n",
        "\n",
        "        self.dataset = dict()\n",
        "        self.dataset[\"local2global_station\"], self.dataset[\"n_station\"] = self.load_local2global_station()\n",
        "        print(\"done load local2global_station...\")\n",
        "        self.dataset[\"local_exist_mask\"] = self.load_local_exist_mask()\n",
        "        print(\"done load exist_mask...\")\n",
        "        self.dataset[\"feature\"] = self.load_feature()\n",
        "        print(\"done load feature...\")\n",
        "        self.dataset[\"month_feature\"] = self.load_month_feat_ls()\n",
        "        print(\"done load month_feature...\")\n",
        "        self.dataset[\"od_feature\"] = self.load_od_feature()\n",
        "        print(\"done load od_feature...\")\n",
        "        self.dataset[\"month_od_feature\"] = self.load_month_od_feat_ls()\n",
        "        print(\"done load month_od_feature...\")\n",
        "        self.dataset[\"month_od_opendays\"] = self.load_month_od_opendays_ls()\n",
        "        print(\"done load month_od_opendays...\")\n",
        "        self.dataset[\"month_od_nonzero\"] = self.load_month_od_nonzero_ls()\n",
        "        print(\"done load month_od_nonzero...\")\n",
        "        self.dataset[\"timestep2month\"] = self.load_month()\n",
        "        self.dataset[\"month_dist_knn\"] = self.load_dist_knn()\n",
        "        self.dataset[\"dist_adj_diag1\"] = self.load_dist_adj_diag1()\n",
        "        self.dataset[\"month_builtEnv_knn\"] = self.load_builtEnv_knn_ls()\n",
        "        self.dataset[\"builtEnv_adj_diag1\"] = self.load_builtEnv_adj_ls_diag1()\n",
        "        self.min_outflow, self.max_outflow = self.compute_min_max_outflow(pre_min=0, pre_max=733.85)\n",
        "        print(\"min_daily_outflow\", self.min_outflow, \"max_daily_outflow\", self.max_outflow)\n",
        "        self.min_od_cnt, self.max_od_cnt, self.min_od_density, self.max_od_density = self.compute_min_max_od()\n",
        "        print(\"min_od_cnt\", self.min_od_cnt, \"max_od_cnt\", self.max_od_cnt)\n",
        "        print(\"min_od_density\", self.min_od_density, \"max_od_density\", self.max_od_density)\n",
        "\n",
        "    def load_timestep_ls(self):\n",
        "        timestep_ls = []\n",
        "        year_ls = [2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
        "        for year in year_ls:\n",
        "          if year == 2013:\n",
        "            month_ls = [\"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
        "          else:\n",
        "            month_ls = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
        "          for month in month_ls:\n",
        "            timestep_ls.append(f\"{year}{month}\")\n",
        "        return timestep_ls\n",
        "\n",
        "    def load_local2global_station(self):\n",
        "        with open(self.local2global_station_dir, 'rb') as handle:\n",
        "          local2global_station_ls = pkl.load(handle)\n",
        "        local2global_station_ls = [torch.from_numpy(local2global_station).long().to(device) for local2global_station in local2global_station_ls]\n",
        "        n_station_ls = [len(local2global_station) for local2global_station in local2global_station_ls]\n",
        "        return local2global_station_ls, torch.LongTensor(n_station_ls).to(device)\n",
        "\n",
        "    def load_local_exist_mask(self):\n",
        "        train_station = set()\n",
        "        local_exist_mask = list()\n",
        "        for i in range(self.n_train_timestep):\n",
        "          train_station = train_station.union(set(self.dataset[\"local2global_station\"][i].tolist()))\n",
        "          local_exist_mask.append(0)\n",
        "        train_station = torch.LongTensor(list(train_station)).to(device)\n",
        "        for local2global_station in self.dataset[\"local2global_station\"][self.n_train_timestep:]:\n",
        "          exist_mask = torch.isin(local2global_station, train_station) * 1\n",
        "          local_exist_mask.append(exist_mask.long().to(device))\n",
        "        return local_exist_mask\n",
        "\n",
        "    def load_feature(self):\n",
        "        feat = pd.read_csv(self.feat_dir)\n",
        "        feat.fillna(0, inplace=True)\n",
        "        feat.drop(columns=[\"station size\", \"size found\"], inplace=True)\n",
        "        feat = feat.values\n",
        "        feat_min, feat_max = feat.min(axis=0), feat.max(axis=0)\n",
        "        feat = (feat - feat_min) / (feat_max - feat_min)\n",
        "        feat = 2 * feat - 1\n",
        "        print(\"feat shape{}\".format(feat.shape))\n",
        "        return torch.from_numpy(feat).float().to(device)\n",
        "\n",
        "    def load_month_feat_ls(self):\n",
        "        with open(self.month_feat_dir, 'rb') as handle:\n",
        "          month_feat_ls = pkl.load(handle)\n",
        "        month_feat_ls = [torch.from_numpy(month_feat).float().to(device) for month_feat in month_feat_ls]\n",
        "        return month_feat_ls\n",
        "\n",
        "    def load_od_feature(self):\n",
        "        \"\"\"od features (travel_dist, geo_dist, bearing)\"\"\"\n",
        "        od_feat = np.load(self.od_feat_dir) # [n_station, n_station, 3]travel_dist, geo_dist, bearing\n",
        "        dist = od_feat[:, :, 0]\n",
        "        np.fill_diagonal(dist, 0)\n",
        "        od_feat[:, :, 0] = dist\n",
        "        \"\"\"dist category features\"\"\"\n",
        "        dist_500 = (dist <= 500) * 1\n",
        "        dist_3000 = (dist > 500) * (dist <= 3000)\n",
        "        dist_5000 = (dist > 3000) * (dist <= 5000)\n",
        "        dist_inf = (dist > 5000) * 1\n",
        "        dist_cat = np.stack([dist_500, dist_3000, dist_5000, dist_inf], -1) # n_s, n_s, 4\n",
        "        \"\"\"mangattan features\"\"\"\n",
        "        manhattan = np.load(self.manhattan_dir)\n",
        "        manhattan_binary = np.zeros((manhattan.shape[0], manhattan.shape[1], 4)) # n_s, n_s, 4\n",
        "        n_station = manhattan_binary.shape[0]\n",
        "        ori_idx = np.stack([np.arange(n_station)] * n_station, 1)\n",
        "        des_idx = np.stack([np.arange(n_station)] * n_station, 0)\n",
        "        manhattan_binary[ori_idx.reshape(-1), des_idx.reshape(-1), manhattan.reshape(-1)] = 1\n",
        "        od_feat = np.concatenate([od_feat, dist_cat, manhattan_binary], -1)\n",
        "        od_min, od_max = np.min(od_feat.min(axis=0), axis=0), np.max(od_feat.max(axis=0), axis=0)\n",
        "        print(np.stack([od_min, od_max]))\n",
        "        od_feat = (od_feat - od_min) / (od_max - od_min)\n",
        "        od_feat = 2 * od_feat - 1\n",
        "        print(\"od feat shape{}\".format(od_feat.shape))\n",
        "        return torch.from_numpy(od_feat).float().to(device)\n",
        "\n",
        "    def load_month_od_feat_ls(self):\n",
        "        new_month_od_feat_ls = []\n",
        "        with open(self.month_od_feat_dir, 'rb') as handle:\n",
        "          month_od_feat_ls = pkl.load(handle)\n",
        "        month_od_feat_ls = [torch.from_numpy(month_od_feat).float().to(device) for month_od_feat in month_od_feat_ls]\n",
        "        return month_od_feat_ls\n",
        "\n",
        "    def load_month_station_opendays_ls(self):\n",
        "        with open(self.month_station_opendays_dir, 'rb') as handle:\n",
        "          month_station_opendays_ls = pkl.load(handle)\n",
        "        month_station_opendays_ls = [torch.from_numpy(month_station_opendays).long().to(device) for \\\n",
        "                                     month_station_opendays in month_station_opendays_ls]\n",
        "        print(len(month_station_opendays_ls))\n",
        "        print(month_station_opendays_ls[0].shape)\n",
        "        return month_station_opendays_ls\n",
        "\n",
        "    def load_month_od_opendays_ls(self):\n",
        "        with open(self.month_od_opendays_dir, 'rb') as handle:\n",
        "          month_od_opendays_ls = pkl.load(handle) # 2, n_station, n_station\n",
        "        if \"daily\" in self.setting:\n",
        "          month_od_opendays_ls = [torch.from_numpy(np.sum(month_od_opendays, 0)).long().to(device) for month_od_opendays in month_od_opendays_ls]\n",
        "        else:\n",
        "          month_od_opendays_ls = [torch.from_numpy(month_od_opendays).long().to(device) for month_od_opendays in month_od_opendays_ls]\n",
        "        return month_od_opendays_ls\n",
        "\n",
        "    def load_month_od_nonzero_ls(self):\n",
        "        with open(self.month_od_nonzero_dir, 'rb') as handle:\n",
        "          month_od_nonzero_ls = pkl.load(handle)\n",
        "        if \"daily\" in self.setting:\n",
        "          new_month_od_nonzero_ls = []\n",
        "          for i, month_od_nonzero in enumerate(month_od_nonzero_ls):\n",
        "            month_od_df = pd.DataFrame(data=month_od_nonzero, columns=[\"weekday\", \"hour\", \"start_sta\", \"end_sta\", \"cnt\"])\n",
        "            month_od_df = month_od_df.groupby([\"start_sta\", \"end_sta\"])[\"cnt\"].sum()\n",
        "            month_od_df.to_csv(\"../data/temp.csv\")\n",
        "            month_od_df = pd.read_csv(\"../data/temp.csv\")\n",
        "            month_od_df.columns=[\"start_sta\", \"end_sta\", \"cnt\"]\n",
        "            new_month_od_nonzero_ls.append(torch.from_numpy(month_od_df.values).long().to(device))\n",
        "        else: # We focus on 8AM-8PM\n",
        "          new_month_od_nonzero_ls = []\n",
        "          for i, month_od_nonzero in enumerate(month_od_nonzero_ls):\n",
        "            month_od_df = pd.DataFrame(data=month_od_nonzero, columns=[\"weekday\", \"hour\", \"start_sta\", \"end_sta\", \"cnt\"])\n",
        "            # remove results from 0-6h\n",
        "            month_od_df = month_od_df.loc[(month_od_df[\"hour\"] >= 8) & (month_od_df[\"hour\"] < 20)].copy()\n",
        "            month_od_df[\"hour\"] = month_od_df[\"hour\"] - 8\n",
        "            month_od_df[self.setting] = np.floor(month_od_df[\"hour\"] / int(self.setting[0])).astype(int)\n",
        "            month_od_df = month_od_df.groupby([\"weekday\", self.setting, \"start_sta\", \"end_sta\"])[\"cnt\"].sum()\n",
        "            month_od_df.to_csv(\"../data/temp.csv\")\n",
        "            month_od_df = pd.read_csv(\"../data/temp.csv\")\n",
        "            month_od_df.columns=[\"weekday\", self.setting, \"start_sta\", \"end_sta\", \"cnt\"]\n",
        "            new_month_od_nonzero_ls.append(torch.from_numpy(month_od_df.values).long().to(device))\n",
        "        del month_od_nonzero_ls\n",
        "        return new_month_od_nonzero_ls\n",
        "\n",
        "    def load_month(self):\n",
        "        time_ls = [6, 7, 8, 9, 10, 11]\n",
        "        for i in range(6):\n",
        "            time_ls = time_ls + [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
        "        time_arr = np.array(time_ls, dtype=np.int32)\n",
        "        print(\"month shape{}\".format(time_arr.shape))\n",
        "        return torch.from_numpy(time_arr).long().to(device)\n",
        "\n",
        "    def load_dist_knn(self):\n",
        "        dist_knn = np.load(self.dist_knn_dir)[:, :, :self.k] # n_month, n_station, k1\n",
        "        local_dist_knn_ls = []\n",
        "        for i, local2global_station in enumerate(self.dataset[\"local2global_station\"]):\n",
        "          local2global_station = local2global_station.detach().cpu().numpy()\n",
        "          local_dist_knn = dist_knn[i, local2global_station] # n_local_station, k\n",
        "          global2local_station = np.zeros(1101, dtype=np.int32)\n",
        "          global2local_station[local2global_station] = np.arange(len(local2global_station))\n",
        "          local_dist_knn = global2local_station[local_dist_knn.reshape(-1)].reshape(-1, self.k)\n",
        "          local_dist_knn_ls.append(torch.from_numpy(local_dist_knn).long().to(device))\n",
        "        del dist_knn\n",
        "        return local_dist_knn_ls\n",
        "\n",
        "    def load_dist_adj_diag1(self):\n",
        "        adj = np.load(self.dist_adj_dir)\n",
        "        np.fill_diagonal(adj, 1)\n",
        "        return torch.from_numpy(adj).float().to(device)\n",
        "\n",
        "    def load_builtEnv_knn_ls(self):\n",
        "        builtEnv_knn = np.load(self.builtEnv_knn_dir)[:, :, :self.k] # n_month, n_station, k1\n",
        "        local_builtEnv_knn_ls = []\n",
        "        for i, local2global_station in enumerate(self.dataset[\"local2global_station\"]):\n",
        "          local2global_station = local2global_station.detach().cpu().numpy()\n",
        "          local_builtEnv_knn = builtEnv_knn[i, local2global_station] # n_local_station, k\n",
        "          global2local_station = np.zeros(1101, dtype=np.int32)\n",
        "          global2local_station[local2global_station] = np.arange(len(local2global_station))\n",
        "          local_builtEnv_knn = global2local_station[local_builtEnv_knn.reshape(-1)].reshape(-1, self.k)\n",
        "          local_builtEnv_knn_ls.append(torch.from_numpy(local_builtEnv_knn).long().to(device))\n",
        "        del builtEnv_knn\n",
        "        return local_builtEnv_knn_ls\n",
        "\n",
        "    def load_builtEnv_adj_ls_diag1(self):\n",
        "        adj = np.load(self.builtEnv_adj_dir)\n",
        "        local_builtEnv_adj_ls = []\n",
        "        for i, local2global_station in enumerate(self.dataset[\"local2global_station\"]):\n",
        "          local2global_station = local2global_station.detach().cpu().numpy()\n",
        "          local_builtEnv_adj = adj[i][local2global_station][:, local2global_station]\n",
        "          np.fill_diagonal(local_builtEnv_adj, 1)\n",
        "          local_builtEnv_adj_ls.append(torch.from_numpy(local_builtEnv_adj).float().to(device))\n",
        "        del adj\n",
        "        return local_builtEnv_adj_ls\n",
        "\n",
        "    def compute_min_max_outflow(self, pre_min=None, pre_max=None):\n",
        "        if pre_min is not None:\n",
        "          return pre_min, pre_max\n",
        "        min_outflow, max_outflow = 0, 0\n",
        "        for month_od_nonzero, month_station_opendays in zip(self.dataset[\"month_od_nonzero\"], self.dataset[\"month_station_opendays\"]):\n",
        "          month_od = np.zeros((month_station_opendays.shape[1], month_station_opendays.shape[1]))\n",
        "          month_od[month_od_nonzero[:, 0], month_od_nonzero[:, 1], month_od_nonzero[:, 2], month_od_nonzero[:, 3]] = month_od_nonzero[:, 4]\n",
        "          month_outflow = np.sum(np.sum(month_od, 1), -1)\n",
        "          month_outflow = month_outflow.astype(float) / (month_station_opendays + 1e-32)\n",
        "          if np.min(month_outflow) < min_outflow:\n",
        "            min_outflow = np.min(month_outflow)\n",
        "          if np.max(month_outflow) > max_outflow:\n",
        "            max_outflow = np.max(month_outflow)\n",
        "        return min_outflow, max_outflow\n",
        "\n",
        "    def compute_min_max_od(self, pre_min=None, pre_max=None):\n",
        "        if pre_min is not None:\n",
        "          return pre_min, pre_max\n",
        "        min_od_density, max_od_density, min_od_cnt, max_od_cnt = 0, 0, 0, 0\n",
        "        for month_od_nonzero, month_od_opendays in zip(self.dataset[\"month_od_nonzero\"][self.n_train_timestep:],\n",
        "                                                       self.dataset[\"month_od_opendays\"][self.n_train_timestep:]):\n",
        "          month_od_nonzero = month_od_nonzero.detach().cpu().numpy()\n",
        "          month_od_opendays = month_od_opendays.detach().cpu().numpy()\n",
        "\n",
        "          month_od = np.zeros((month_od_opendays.shape[1], month_od_opendays.shape[1]))\n",
        "          month_od[month_od_nonzero[:, 0], month_od_nonzero[:, 1]] = month_od_nonzero[:, 2]\n",
        "          if np.min(month_od) < min_od_cnt:\n",
        "            min_od_cnt = np.min(month_od)\n",
        "          if np.max(month_od) > max_od_cnt:\n",
        "            max_od_cnt = np.max(month_od)\n",
        "\n",
        "          month_od = month_od / (month_od_opendays + 1e-32)\n",
        "          if np.min(month_od) < min_od_density:\n",
        "            min_od_density = np.min(month_od)\n",
        "          if np.max(month_od) > max_od_density:\n",
        "            max_od_density = np.max(month_od)\n",
        "        return min_od_cnt, max_od_cnt, min_od_density, max_od_density\n",
        "\n",
        "    def minmax_normalize(self, x, min, max):\n",
        "        x = (x - min) / (max - min)\n",
        "        x = 2 * x - 1\n",
        "        return x\n",
        "\n",
        "    def minmax_denormalize(self, x, min, max):\n",
        "        x = (x + 1) / 2\n",
        "        x = (max - min) * x + min\n",
        "        return x\n",
        "\n",
        "class DataGenerator(object):\n",
        "    def __init__(self, data_class):\n",
        "        self.data_class = data_class\n",
        "\n",
        "    def get_data_loader(self, batch_size: int):\n",
        "        data_loader = dict()\n",
        "        for mode in ['train', 'test']:\n",
        "            samples = PrepareSample(device=device, n_station_ls=self.data_class.dataset[\"n_station\"], n_train_timestep=self.data_class.n_train_timestep,\n",
        "                                    mode=mode)\n",
        "            if mode == 'train':\n",
        "                data_loader['train'] = DataLoader(dataset=PrepareDataset(device=device, inputs=samples.inputs, output=samples.output), \\\n",
        "                                               batch_size=batch_size, shuffle=True)\n",
        "                data_loader['valid'] = DataLoader(dataset=PrepareDataset(device=device, inputs=samples.val_inputs, output=samples.val_output), \\\n",
        "                                               batch_size=batch_size, shuffle=False)\n",
        "            else:\n",
        "                data_loader['test'] = DataLoader(dataset=PrepareDataset(device=device, inputs=samples.inputs, output=samples.output), \\\n",
        "                                               batch_size=batch_size, shuffle=False)\n",
        "        return data_loader\n",
        "\n",
        "class PrepareSample(object):\n",
        "    def __init__(self, device: str, n_station_ls, n_train_timestep, mode):\n",
        "          self.device = device\n",
        "          self.n_station_ls = n_station_ls\n",
        "          self.n_train_timestep = n_train_timestep\n",
        "          self.mode = mode\n",
        "          self.inputs, self.output, self.val_inputs, self.val_output = None, None, None, None\n",
        "          self.prepare_xy()\n",
        "\n",
        "    def prepare_xy(self):\n",
        "        \"\"\"origin feat\"\"\"\n",
        "        # 2, 18, n_station\n",
        "        timestep, local_station = [], []\n",
        "        if self.mode == \"train\":\n",
        "          min_snapshot, max_snapshot = 0, self.n_train_timestep\n",
        "        else:\n",
        "          min_snapshot, max_snapshot = self.n_train_timestep, len(self.n_station_ls)\n",
        "        for i in range(min_snapshot, max_snapshot):\n",
        "          timestep.append(np.array([i] * self.n_station_ls[i].item()))\n",
        "          local_station.append(np.arange(self.n_station_ls[i].item()))\n",
        "        timestep = np.concatenate(timestep, 0)\n",
        "        local_station = np.concatenate(local_station, 0)\n",
        "        if self.mode == \"train\":\n",
        "          val_idx = np.load(\"../data/val_idx.npy\")\n",
        "          train_idx = np.ones_like(timestep)\n",
        "          train_idx[val_idx] = 0\n",
        "          train_idx = np.where(train_idx == 1)[0]\n",
        "          print(\"val_idx\", len(val_idx), \"train_idx\", len(train_idx))\n",
        "          for mode in [\"train\", \"valid\"]:\n",
        "              idx = val_idx if mode == 'valid' else train_idx\n",
        "              x_timestep = timestep[idx]\n",
        "              x_local_station = local_station[idx]\n",
        "              x, y = dict(), dict()\n",
        "              x['timestep'] = torch.from_numpy(x_timestep).long().to(device)\n",
        "              x['local_station'] = torch.from_numpy(x_local_station).long().to(device)\n",
        "              print(f\"==================={mode}======================\")\n",
        "              print('timestep', x['timestep'].shape)\n",
        "              # print('day', x['day'].shape)\n",
        "              print('local_station', x['local_station'].shape)\n",
        "              if mode == \"train\":\n",
        "                  self.inputs, self.output = x, y\n",
        "              else:\n",
        "                  self.val_inputs, self.val_output = x, y\n",
        "        else:\n",
        "          x, y = dict(), dict()\n",
        "          x['timestep'] = torch.from_numpy(timestep).long().to(device)\n",
        "          x['local_station'] = torch.from_numpy(local_station).long().to(device)\n",
        "          print(f\"===================test======================\")\n",
        "          print('timestep', x['timestep'].shape)\n",
        "          print('local_station', x['local_station'].shape)\n",
        "          self.inputs, self.output = x, y\n",
        "\n",
        "\n",
        "class PrepareDataset(Dataset):\n",
        "    def __init__(self, device: str, inputs: dict, output: dict):\n",
        "        self.device = device\n",
        "        self.inputs, self.output = inputs, output\n",
        "        print(\"timestep\", torch.is_tensor(self.inputs['timestep']))\n",
        "        print(\"local_station\", torch.is_tensor(self.inputs['local_station']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.inputs['timestep'].shape[0]\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.inputs['timestep'][item], self.inputs['local_station'][item]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhiNvWtZKgQJ"
      },
      "source": [
        "# Model Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uKXxDir-ffC1"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class ModelTrainer(object):\n",
        "    def __init__(self, hour_prob_model_name: str, hour_prob_model: nn.Module, hour_prob_epochs: int,\n",
        "              optimizer, lr:float, wd:float, dataset, data_class, alpha=0.01):\n",
        "        self.hour_prob_model_name = hour_prob_model_name\n",
        "        self.hour_prob_model = hour_prob_model\n",
        "        self.hour_prob_optimizer = optimizer(params=self.hour_prob_model.parameters(), lr=lr, weight_decay=wd)\n",
        "        self.alpha = alpha\n",
        "        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n",
        "        self.hour_prob_epochs = hour_prob_epochs\n",
        "        self.data_class = data_class\n",
        "        self.dataset = dataset # n_station, n_feat1\n",
        "        self.setting = self.hour_prob_model.setting\n",
        "        self.dist = self.hour_prob_model.dist\n",
        "\n",
        "    def prepare_graph_daily_density_batch_data(self, timestep_idx, local_station_idx, dim_station_feat=44, dim_od_feat=14):\n",
        "        batch_size, n_max_station = timestep_idx.shape[0], torch.max(self.dataset[\"n_station\"][timestep_idx])\n",
        "        n_neigh = self.data_class.k\n",
        "        \"\"\"ori_feature\"\"\"\n",
        "        batch_ori_feature = torch.zeros((batch_size, dim_station_feat)).float().to(device)\n",
        "        batch_ori_dist_feature = torch.zeros((batch_size, n_neigh, dim_station_feat)).float().to(device)\n",
        "        batch_ori_dist_adj = torch.zeros((batch_size, n_neigh)).float().to(device)\n",
        "        batch_ori_builtEnv_feature = torch.zeros((batch_size, n_neigh, dim_station_feat)).float().to(device)\n",
        "        batch_ori_builtEnv_adj = torch.zeros((batch_size, n_neigh)).float().to(device)\n",
        "        \"\"\"des_feature\"\"\"\n",
        "        batch_des_feature = torch.zeros((batch_size, n_max_station, dim_station_feat)).float().to(device)\n",
        "        batch_des_dist_feature = torch.zeros((batch_size, n_max_station, n_neigh, dim_station_feat)).float().to(device)\n",
        "        batch_des_dist_adj = torch.zeros((batch_size, n_max_station, n_neigh)).float().to(device)\n",
        "        batch_des_builtEnv_feature = torch.zeros((batch_size, n_max_station, n_neigh, dim_station_feat)).float().to(device)\n",
        "        batch_des_builtEnv_adj = torch.zeros((batch_size, n_max_station, n_neigh)).float().to(device)\n",
        "        \"\"\"od_feature\"\"\"\n",
        "        batch_od_feature = torch.zeros((batch_size, n_max_station, dim_od_feat-1)).float().to(device)\n",
        "        batch_od_opendays = torch.zeros((batch_size, n_max_station)).long().to(device)\n",
        "        \"\"\"od_dist_feature\"\"\"\n",
        "        batch_od_dist_feature = torch.zeros((batch_size, n_max_station, (n_neigh+1)*(n_neigh+1)-1, dim_od_feat-1)).float().to(device)\n",
        "        # batch_od_dist_opendays = torch.zeros((batch_size, n_max_station, (n_neigh+1)*(n_neigh+1)-1)).float().to(device)\n",
        "        batch_od_dist_adj = torch.zeros((batch_size, n_max_station, (n_neigh+1)*(n_neigh+1)-1)).to(device)\n",
        "        \"\"\"od_builtEnv_feature\"\"\"\n",
        "        batch_od_builtEnv_feature = torch.zeros((batch_size, n_max_station, (n_neigh+1)*(n_neigh+1)-1, dim_od_feat-1)).float().to(device)\n",
        "        # batch_od_builtEnv_opendays = torch.zeros((batch_size, n_max_station, (n_neigh+1)*(n_neigh+1)-1)).float().to(device)\n",
        "        batch_od_builtEnv_adj = torch.zeros((batch_size, n_max_station, (n_neigh+1)*(n_neigh+1)-1)).to(device)\n",
        "        # batch_prob_des_mask = torch.zeros((batch_size, 16, n_max_station)).long().to(device)\n",
        "        batch_des_mask = torch.zeros((batch_size, n_max_station)).long().to(device)\n",
        "        \"\"\"outflow_inflow_feature\"\"\"\n",
        "        batch_outflow = torch.zeros(batch_size).float().to(device)\n",
        "        batch_inflow = torch.zeros((batch_size, n_max_station)).float().to(device)\n",
        "        batch_od = torch.zeros((batch_size, n_max_station)).float().to(device)\n",
        "        for i, (timestep, local_station) in enumerate(zip(timestep_idx.tolist(), local_station_idx.tolist())):\n",
        "          global_station = self.dataset[\"local2global_station\"][timestep]\n",
        "          \"\"\"ori_feature\"\"\"\n",
        "          x_ori_feature = self.dataset[\"feature\"][global_station[local_station]] # n_feat\n",
        "          x_ori_month_feature = self.dataset[\"month_feature\"][timestep][local_station] # n_month_feat\n",
        "          batch_ori_feature[i] = torch.cat([x_ori_feature, x_ori_month_feature], -1) # batch_size, n_feat\n",
        "          \"\"\"ori_dist_neigh\"\"\"\n",
        "          ori_dist_knn = self.dataset[\"month_dist_knn\"][timestep][local_station]\n",
        "          x_ori_dist_feature = self.dataset[\"feature\"][global_station[ori_dist_knn]]\n",
        "          x_ori_dist_month_feature = self.dataset[\"month_feature\"][timestep][ori_dist_knn]\n",
        "          batch_ori_dist_feature[i] = torch.cat([x_ori_dist_feature, x_ori_dist_month_feature], -1)\n",
        "          batch_ori_dist_adj[i] = self.dataset[\"dist_adj_diag1\"][global_station[local_station], global_station[ori_dist_knn]]\n",
        "          \"\"\"ori_builtEnv_neigh\"\"\"\n",
        "          ori_builtEnv_knn = self.dataset[\"month_builtEnv_knn\"][timestep][local_station]\n",
        "          x_ori_builtEnv_feature = self.dataset[\"feature\"][global_station[ori_builtEnv_knn]]\n",
        "          x_ori_builtEnv_month_feature = self.dataset[\"month_feature\"][timestep][ori_builtEnv_knn]\n",
        "          batch_ori_builtEnv_feature[i] = torch.cat([x_ori_builtEnv_feature, x_ori_builtEnv_month_feature], -1)\n",
        "          batch_ori_builtEnv_adj[i] = self.dataset[\"builtEnv_adj_diag1\"][timestep][local_station, ori_dist_knn]\n",
        "          \"\"\"des_feature\"\"\"\n",
        "          x_des_feature = self.dataset[\"feature\"][global_station] # n_local_station, n_feat\n",
        "          x_des_month_feature = self.dataset[\"month_feature\"][timestep] # n_local_station, n_month_feat\n",
        "          batch_des_feature[i, :len(global_station)] = torch.cat([x_des_feature, x_des_month_feature], -1)\n",
        "          \"\"\"des_dist_neigh\"\"\"\n",
        "          des_dist_knn = self.dataset[\"month_dist_knn\"][timestep] # n_local_station, k\n",
        "          x_des_dist_feature = self.dataset[\"feature\"][global_station[des_dist_knn].reshape(-1)].reshape(len(global_station), n_neigh, -1)\n",
        "          x_des_dist_month_feature = self.dataset[\"month_feature\"][timestep][des_dist_knn.reshape(-1)].reshape(len(global_station), n_neigh, -1)\n",
        "          batch_des_dist_feature[i, :len(global_station)] = torch.cat([x_des_dist_feature, x_des_dist_month_feature], -1)\n",
        "          batch_des_dist_adj[i, :len(global_station)] = self.dataset[\"dist_adj_diag1\"][global_station.unsqueeze(-1).repeat(1, n_neigh).reshape(-1), \\\n",
        "                                        global_station[des_dist_knn.reshape(-1)]].reshape(len(global_station), n_neigh)\n",
        "          \"\"\"des_builtEnv_neigh\"\"\"\n",
        "          station_arange_idx = torch.arange(len(global_station)).to(device)\n",
        "          des_builtEnv_knn = self.dataset[\"month_builtEnv_knn\"][timestep] # n_local_station, k\n",
        "          x_des_builtEnv_feature = self.dataset[\"feature\"][global_station[des_builtEnv_knn].reshape(-1)].reshape(len(global_station), n_neigh, -1)\n",
        "          x_des_builtEnv_month_feature = self.dataset[\"month_feature\"][timestep][des_builtEnv_knn.reshape(-1)].reshape(len(global_station), n_neigh, -1)\n",
        "          batch_des_builtEnv_feature[i, :len(global_station)] = torch.cat([x_des_builtEnv_feature, x_des_builtEnv_month_feature], -1)\n",
        "          batch_des_builtEnv_adj[i, :len(global_station)] = self.dataset[\"builtEnv_adj_diag1\"][timestep][station_arange_idx.unsqueeze(-1).repeat(1, n_neigh).reshape(-1), \\\n",
        "                                        des_builtEnv_knn.reshape(-1)].reshape(len(global_station), n_neigh)\n",
        "          \"\"\"od_dist_neigh\"\"\"\n",
        "          ori_target_neigh_knn = torch.LongTensor([local_station] + ori_dist_knn.tolist()).to(device) # n_neigh + 1\n",
        "          ori_target_neigh_knn = ori_target_neigh_knn.unsqueeze(0).repeat(len(global_station), 1) # n_station, n_neigh + 1\n",
        "          des_target_neigh_knn = torch.cat([station_arange_idx.unsqueeze(-1), des_dist_knn], -1) # n_station, n_neigh + 1\n",
        "          ori_target_neigh_knn = ori_target_neigh_knn.unsqueeze(-1).repeat(1, 1, n_neigh+1)\n",
        "          des_target_neigh_knn = des_target_neigh_knn.unsqueeze(-2).repeat(1, n_neigh+1, 1)\n",
        "          x_od_feature = self.dataset[\"od_feature\"][global_station[ori_target_neigh_knn.reshape(-1)], \\\n",
        "                                    global_station[des_target_neigh_knn.reshape(-1)]]\n",
        "          x_od_feature = x_od_feature.reshape(len(global_station), (n_neigh+1)*(n_neigh+1), -1) # n_local_station, n_od_feat\n",
        "          x_od_month_feature = self.dataset[\"month_od_feature\"][timestep][ori_target_neigh_knn.reshape(-1), des_target_neigh_knn.reshape(-1)] # n_local_station, n_od_month_feat\n",
        "          x_od_month_feature = x_od_month_feature.reshape(len(global_station), (n_neigh+1)*(n_neigh+1), -1)\n",
        "          x_od_feature = torch.cat([x_od_feature, x_od_month_feature], -1)\n",
        "          batch_od_feature[i, :len(global_station)] = x_od_feature[:, 0]\n",
        "          batch_od_dist_feature[i, :len(global_station)] = x_od_feature[:, 1:]\n",
        "          ori_target_neigh_adj = self.dataset[\"dist_adj_diag1\"][global_station[local_station], global_station[ori_target_neigh_knn.reshape(-1)]]\n",
        "          des_target_neigh_adj = self.dataset[\"dist_adj_diag1\"][global_station[station_arange_idx.unsqueeze(-1).unsqueeze(-1).repeat(1, n_neigh+1,n_neigh+1).reshape(-1)],\\\n",
        "                                      global_station[des_target_neigh_knn.reshape(-1)]]\n",
        "          od_target_neigh_adj = (ori_target_neigh_adj + des_target_neigh_adj) / 2\n",
        "          batch_od_dist_adj[i, :len(global_station)] = od_target_neigh_adj.reshape(len(global_station), (n_neigh+1)*(n_neigh+1))[:, 1:]\n",
        "          \"\"\"od_builtEnv_neigh\"\"\"\n",
        "          ori_target_neigh_knn = torch.LongTensor([local_station] + ori_builtEnv_knn.tolist()).to(device) # n_neigh + 1\n",
        "          ori_target_neigh_knn = ori_target_neigh_knn.unsqueeze(0).repeat(len(global_station), 1) # n_station, n_neigh + 1\n",
        "          des_target_neigh_knn = torch.cat([station_arange_idx.unsqueeze(-1), des_builtEnv_knn], -1) # n_station, n_neigh + 1\n",
        "          ori_target_neigh_knn = ori_target_neigh_knn.unsqueeze(-1).repeat(1, 1, n_neigh+1)\n",
        "          des_target_neigh_knn = des_target_neigh_knn.unsqueeze(-2).repeat(1, n_neigh+1, 1)\n",
        "          x_od_feature = self.dataset[\"od_feature\"][global_station[ori_target_neigh_knn.reshape(-1)], \\\n",
        "                                    global_station[des_target_neigh_knn.reshape(-1)]]\n",
        "          x_od_feature = x_od_feature.reshape(len(global_station), (n_neigh+1)*(n_neigh+1), -1) # n_local_station, n_od_feat\n",
        "          x_od_month_feature = self.dataset[\"month_od_feature\"][timestep][ori_target_neigh_knn.reshape(-1), des_target_neigh_knn.reshape(-1)] # n_local_station, n_od_month_feat\n",
        "          x_od_month_feature = x_od_month_feature.reshape(len(global_station), (n_neigh+1)*(n_neigh+1), -1)\n",
        "          x_od_feature = torch.cat([x_od_feature, x_od_month_feature], -1)\n",
        "          batch_od_builtEnv_feature[i, :len(global_station)] = x_od_feature[:, 1:]\n",
        "          ori_target_neigh_adj = self.dataset[\"builtEnv_adj_diag1\"][timestep][local_station, ori_target_neigh_knn.reshape(-1)]\n",
        "          des_target_neigh_adj = self.dataset[\"builtEnv_adj_diag1\"][timestep][station_arange_idx.unsqueeze(-1).unsqueeze(-1).repeat(1, n_neigh+1,n_neigh+1).reshape(-1),\\\n",
        "                                      des_target_neigh_knn.reshape(-1)]\n",
        "          od_target_neigh_adj = (ori_target_neigh_adj + des_target_neigh_adj) / 2\n",
        "          batch_od_builtEnv_adj[i, :len(global_station)] = od_target_neigh_adj.reshape(len(global_station), (n_neigh+1)*(n_neigh+1))[:, 1:]\n",
        "\n",
        "          \"\"\"y\"\"\"\n",
        "          y_od_nonzero = self.dataset[\"month_od_nonzero\"][timestep]\n",
        "          y_od = torch.zeros((len(global_station), len(global_station))).long().to(device)\n",
        "          y_od[y_od_nonzero[:, 0], y_od_nonzero[:, 1]] = y_od_nonzero[:, 2]\n",
        "          y_od_opendays = self.dataset[\"month_od_opendays\"][timestep]\n",
        "          batch_outflow[i] = torch.sum(y_od[local_station])\n",
        "          batch_inflow[i, :len(global_station)] = torch.sum(y_od, 0)\n",
        "          batch_od[i, :len(global_station)] = y_od[local_station]\n",
        "          batch_od_opendays[i, :len(global_station)] = y_od_opendays[local_station]\n",
        "          batch_des_mask[i, :len(global_station)] = 1\n",
        "        batch_month = self.dataset[\"timestep2month\"][timestep_idx]\n",
        "        return batch_ori_feature, batch_ori_dist_feature, batch_ori_dist_adj, batch_ori_builtEnv_feature, batch_ori_builtEnv_adj, \\\n",
        "            batch_des_feature, batch_des_dist_feature, batch_des_dist_adj, batch_des_builtEnv_feature, batch_des_builtEnv_adj, \\\n",
        "            batch_od_feature, batch_od_dist_feature, batch_od_dist_adj, batch_od_builtEnv_feature, batch_od_builtEnv_adj, \\\n",
        "            batch_od, batch_od_opendays, batch_outflow, batch_inflow, \\\n",
        "            batch_des_mask, batch_month\n",
        "\n",
        "    def prepare_ori_evaluation_mask(self, timestep_idx, day_idx, local_station_idx):\n",
        "        exist_mask = [self.dataset[\"local_exist_mask\"][timestep][local_station] for timestep, local_station in \\\n",
        "                      zip(timestep_idx.tolist(), local_station_idx.tolist())]\n",
        "        return torch.LongTensor(exist_mask).to(device)\n",
        "\n",
        "    def prepare_od_evaluation_mask(self, timestep_idx, local_station_idx):\n",
        "        batch_size, n_max_station = timestep_idx.shape[0], torch.max(self.dataset[\"n_station\"][timestep_idx])\n",
        "        batch_exist_exist_mask = -torch.ones((batch_size, n_max_station)).long().to(device)\n",
        "        batch_exist_add_mask = -torch.ones((batch_size, n_max_station)).long().to(device)\n",
        "        batch_add_add_mask = -torch.ones((batch_size, n_max_station)).long().to(device)\n",
        "        for i, (timestep, local_station) in enumerate(zip(timestep_idx.tolist(), local_station_idx.tolist())):\n",
        "          local_exist_mask = self.dataset[\"local_exist_mask\"][timestep]\n",
        "          exist_ori = local_exist_mask[local_station]\n",
        "          batch_exist_exist_mask[i, :len(local_exist_mask)] = exist_ori * local_exist_mask\n",
        "          batch_exist_add_mask[i, :len(local_exist_mask)] = ((exist_ori * (1 - local_exist_mask) + (1 - exist_ori) * local_exist_mask) > 0) * 1\n",
        "          batch_add_add_mask[i, :len(local_exist_mask)] = (1 - exist_ori) * (1 - local_exist_mask)\n",
        "        return batch_exist_exist_mask, batch_exist_add_mask, batch_add_add_mask\n",
        "\n",
        "\n",
        "    def train_hour_od(self, data_processor:dict, modes:list, model_dir:str, early_stopper=10, n_sample=100, mask_zero_rate=0.5,\n",
        "                        weight=0, reg=0):\n",
        "        checkpoint = {'epoch':0, 'state_dict':self.hour_prob_model.state_dict()}\n",
        "        val_loss = np.inf\n",
        "        outflow_loss = np.inf\n",
        "        inflow_loss = np.inf\n",
        "        start_time = datetime.datetime.now()\n",
        "        best_result = {\"All\": {\"CPC\": 0.}}\n",
        "        for epoch in range(1, self.hour_prob_epochs+1):\n",
        "            running_loss = {mode:0.0 for mode in modes}\n",
        "            outflow_running_loss = {mode:0.0 for mode in modes}\n",
        "            inflow_running_loss = {mode:0.0 for mode in modes}\n",
        "            for mode in modes:\n",
        "                if mode == 'train':\n",
        "                    self.hour_prob_model.train()\n",
        "                else:\n",
        "                    self.hour_prob_model.eval()\n",
        "                step = 0\n",
        "                curr_sample = 0\n",
        "                for timestep, local_station in data_processor[mode]:\n",
        "                    ori_feat, ori_dist_feat, ori_dist_adj, ori_builtEnv_feat, ori_builtEnv_adj, \\\n",
        "                          des_feat, des_dist_feat, des_dist_adj, des_builtEnv_feat, des_builtEnv_adj, \\\n",
        "                          od_feat, od_dist_feat, od_dist_adj, od_builtEnv_feat, od_builtEnv_adj, \\\n",
        "                          y_od, y_od_opendays, y_outflow, y_inflow, des_mask, x_month = \\\n",
        "                          self.prepare_graph_daily_density_batch_data(timestep, local_station)\n",
        "                    des_mask = (y_od_opendays > 0) * 1\n",
        "                    if self.setting == \"daily_density\":\n",
        "                        y_od = y_od / (y_od_opendays + 1e-32)\n",
        "                    else:\n",
        "                        od_feat = torch.cat([od_feat, (y_od_opendays / 31.).unsqueeze(-1)], -1)\n",
        "                    with torch.set_grad_enabled(mode = mode=='train'):\n",
        "                        if self.dist ==\"zinb\":\n",
        "                            n_train, p_train, pi_train = \\\n",
        "                                self.hour_prob_model(ori_feat, ori_dist_feat, ori_dist_adj, ori_builtEnv_feat, ori_builtEnv_adj, \\\n",
        "                                            des_feat, des_dist_feat, des_dist_adj, des_builtEnv_feat, des_builtEnv_adj, \\\n",
        "                                            od_feat, od_dist_feat, od_dist_adj, od_builtEnv_feat, od_builtEnv_adj, \\\n",
        "                                            x_month, des_mask)\n",
        "                            loss = nb_zeroinflated_nll_loss(y_od, n_train, p_train, pi_train, des_mask, weight=weight)\n",
        "                        elif self.dist == \"nb\":\n",
        "                            n_train, p_train = \\\n",
        "                                self.hour_prob_model(ori_feat, ori_dist_feat, ori_dist_adj, ori_builtEnv_feat, ori_builtEnv_adj, \\\n",
        "                                            des_feat, des_dist_feat, des_dist_adj, des_builtEnv_feat, des_builtEnv_adj, \\\n",
        "                                            od_feat, od_dist_feat, od_dist_adj, od_builtEnv_feat, od_builtEnv_adj, \\\n",
        "                                            x_month, des_mask)\n",
        "                            loss = nb_nll_loss(y_od, n_train, p_train, des_mask)\n",
        "                        else:\n",
        "                            y_pred = self.hour_prob_model(ori_feat, ori_dist_feat, ori_dist_adj, ori_builtEnv_feat, ori_builtEnv_adj, \\\n",
        "                                            des_feat, des_dist_feat, des_dist_adj, des_builtEnv_feat, des_builtEnv_adj, \\\n",
        "                                            od_feat, od_dist_feat, od_dist_adj, od_builtEnv_feat, od_builtEnv_adj, \\\n",
        "                                            x_month, des_mask)\n",
        "                            loss = self.mse_loss(y_pred[des_mask > 0], y_od[des_mask > 0])\n",
        "                        if mode == 'train':\n",
        "                            self.hour_prob_optimizer.zero_grad()\n",
        "                            loss.backward()\n",
        "                            self.hour_prob_optimizer.step()\n",
        "                    running_loss[mode] += loss\n",
        "                    step += y_od.shape[0]\n",
        "                    curr_sample += 1\n",
        "                    if curr_sample == n_sample: break\n",
        "                if mode == 'valid':\n",
        "                    if running_loss[mode] / step <= val_loss:\n",
        "                        print(f'Epoch {epoch}, Val_loss drops from {val_loss:.5} to {running_loss[mode] / step:.5}. '\n",
        "                              f'Update model checkpoint..')\n",
        "                        val_loss = running_loss[mode] / step\n",
        "                        checkpoint.update(epoch=epoch, state_dict=self.hour_prob_model.state_dict())\n",
        "                        torch.save(checkpoint, model_dir + f'/{self.hour_prob_model_name}_best_model.pkl')\n",
        "                        early_stopper = 10\n",
        "                    else:\n",
        "                        print(f'Epoch {epoch}, Val_loss does not improve from {val_loss:.5}.')\n",
        "                        early_stopper -= 1\n",
        "                        if early_stopper == 0:\n",
        "                            print(f'Early stopping at epoch {epoch}..')\n",
        "                            return\n",
        "                    saved_checkpoint_od = torch.load(model_dir + f'/{self.hour_prob_model_name}_best_model.pkl')\n",
        "                    self.hour_prob_model.load_state_dict(saved_checkpoint_od['state_dict'])\n",
        "            # if epoch % 1 == 0:\n",
        "            #     result = self.test_hour_od(epoch=epoch, data_processor=data_processor, modes=['test'], model_dir=model_dir)\n",
        "        print('training', datetime.datetime.now() - start_time)\n",
        "        return\n",
        "\n",
        "    def test_batch_od(self, ground_truth, prediction, n_test=None, p_test=None, pi_test=None):\n",
        "        \"\"\"exist_exist\"\"\"\n",
        "        SE = self.batch_SE(prediction, ground_truth)\n",
        "        AE = self.batch_AE(prediction, ground_truth)\n",
        "        batch_CPC_up, batch_CPC_bottom = self.batch_CPC(prediction, ground_truth)\n",
        "        CPC_up = batch_CPC_up\n",
        "        CPC_bottom = batch_CPC_bottom\n",
        "        if self.dist==\"zinb\":\n",
        "            MPIW, PICP = self.batch_zinb_MPIW(n_test, p_test, pi_test, ground_truth)\n",
        "        elif self.dist==\"nb\":\n",
        "            MPIW, PICP = self.batch_nb_MPIW(n_test, p_test, ground_truth)\n",
        "        else:\n",
        "            MPIW, PICP = 0, 0\n",
        "        n_sample = len(ground_truth)\n",
        "        return [SE, AE, CPC_up, CPC_bottom, n_sample, MPIW, PICP]\n",
        "\n",
        "    def test_hour_od(self, epoch, data_processor:dict, modes:list, model_dir:str):\n",
        "        saved_checkpoint_od = torch.load(model_dir + f'/{self.hour_prob_model_name}_best_model.pkl', map_location=torch.device('cpu'))\n",
        "        self.hour_prob_model.load_state_dict(saved_checkpoint_od['state_dict'])\n",
        "        self.hour_prob_model.eval()\n",
        "        running_loss = {mode: 0.0 for mode in modes}\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        for mode in modes:\n",
        "            metric_dict = {}\n",
        "            flow_settings = [\"all\", \"od=0\", \"od>0\", \"od>=1\", \"od>=5\"]\n",
        "            od_settings = [\"ee\", \"ea\", \"aa\"]\n",
        "            metric_names = [\"SE\", \"AE\", \"CPC_up\", \"CPC_bottom\", \"n_sample\", \"MPIW\", \"PICP\"]\n",
        "            for flow_setting in flow_settings:\n",
        "              metric_dict[flow_setting] = dict()\n",
        "              for od_setting in od_settings:\n",
        "                metric_dict[flow_setting][od_setting] = dict()\n",
        "                for metric_name in metric_names:\n",
        "                  metric_dict[flow_setting][od_setting][metric_name] = 0\n",
        "            for timestep, local_station in data_processor[mode]:\n",
        "                ori_feat, ori_dist_feat, ori_dist_adj, ori_builtEnv_feat, ori_builtEnv_adj, \\\n",
        "                      des_feat, des_dist_feat, des_dist_adj, des_builtEnv_feat, des_builtEnv_adj, \\\n",
        "                      od_feat, od_dist_feat, od_dist_adj, od_builtEnv_feat, od_builtEnv_adj, \\\n",
        "                      y_true, y_od_opendays, y_outflow, y_inflow, des_mask, x_month = \\\n",
        "                      self.prepare_graph_daily_density_batch_data(timestep, local_station)\n",
        "                des_mask = (y_od_opendays > 0) * 1\n",
        "                if self.setting == \"daily_density\":\n",
        "                    y_true = y_true / (y_od_opendays + 1e-32)\n",
        "                else:\n",
        "                    od_feat = torch.cat([od_feat, (y_od_opendays / 31.).unsqueeze(-1)], -1)\n",
        "\n",
        "                if self.dist ==\"zinb\":\n",
        "                    n_test, p_test, pi_test = \\\n",
        "                        self.hour_prob_model(ori_feat, ori_dist_feat, ori_dist_adj, ori_builtEnv_feat, ori_builtEnv_adj, \\\n",
        "                                    des_feat, des_dist_feat, des_dist_adj, des_builtEnv_feat, des_builtEnv_adj, \\\n",
        "                                    od_feat, od_dist_feat, od_dist_adj, od_builtEnv_feat, od_builtEnv_adj, \\\n",
        "                                    x_month, des_mask)\n",
        "                    mean_pred = (1 - pi_test)*(n_test/p_test - n_test)\n",
        "                elif self.dist ==\"nb\":\n",
        "                    n_test, p_test = \\\n",
        "                        self.hour_prob_model(ori_feat, ori_dist_feat, ori_dist_adj, ori_builtEnv_feat, ori_builtEnv_adj, \\\n",
        "                                    des_feat, des_dist_feat, des_dist_adj, des_builtEnv_feat, des_builtEnv_adj, \\\n",
        "                                    od_feat, od_dist_feat, od_dist_adj, od_builtEnv_feat, od_builtEnv_adj, \\\n",
        "                                    x_month, des_mask)\n",
        "                    mean_pred = n_test/p_test\n",
        "                else:\n",
        "                    y_pred = self.hour_prob_model(ori_feat, ori_dist_feat, ori_dist_adj, ori_builtEnv_feat, ori_builtEnv_adj, \\\n",
        "                                    des_feat, des_dist_feat, des_dist_adj, des_builtEnv_feat, des_builtEnv_adj, \\\n",
        "                                    od_feat, od_dist_feat, od_dist_adj, od_builtEnv_feat, od_builtEnv_adj, \\\n",
        "                                    x_month, des_mask)\n",
        "                    y_pred[y_pred < 0] = 0\n",
        "                    mean_pred = y_pred\n",
        "                exist_exist_mask, exist_add_mask, add_add_mask = self.prepare_od_evaluation_mask(timestep, local_station)\n",
        "                for od_setting in od_settings:\n",
        "                  if od_setting == \"ee\":\n",
        "                    tmp_mask = exist_exist_mask * des_mask\n",
        "                  elif od_setting == \"ea\":\n",
        "                    tmp_mask = exist_add_mask * des_mask\n",
        "                  elif od_setting == \"aa\":\n",
        "                    tmp_mask = add_add_mask * des_mask\n",
        "                  else:\n",
        "                    tmp_mask = des_mask\n",
        "                  for flow_setting in flow_settings:\n",
        "                    if flow_setting == \"od=0\":\n",
        "                      mask = tmp_mask * (y_true==0)\n",
        "                    elif flow_setting == \"od>0\":\n",
        "                      mask = tmp_mask * (y_true>0)\n",
        "                    elif flow_setting == \"od>=1\":\n",
        "                      mask = tmp_mask * (y_true>=1)\n",
        "                    elif flow_setting == \"od>=5\":\n",
        "                      mask = tmp_mask * (y_true>=5)\n",
        "                    else:\n",
        "                      mask = tmp_mask\n",
        "\n",
        "                    if self.dist == \"zinb\":\n",
        "                      metrics = self.test_batch_od(y_true[mask > 0].detach().cpu().numpy().flatten(),\n",
        "                                      mean_pred[mask > 0].detach().cpu().numpy().flatten(),\n",
        "                                      n_test[mask > 0].detach().cpu().numpy().flatten(),\n",
        "                                      p_test[mask > 0].detach().cpu().numpy().flatten(),\n",
        "                                      pi_test[mask > 0].detach().cpu().numpy().flatten())\n",
        "                    elif self.dist == \"nb\":\n",
        "                      metrics = self.test_batch_od(y_true[mask > 0].detach().cpu().numpy().flatten(),\n",
        "                                      mean_pred[mask > 0].detach().cpu().numpy().flatten(),\n",
        "                                      n_test[mask > 0].detach().cpu().numpy().flatten(),\n",
        "                                      p_test[mask > 0].detach().cpu().numpy().flatten())\n",
        "                    else:\n",
        "                      metrics = self.test_batch_od(y_true[mask > 0].detach().cpu().numpy().flatten(),\n",
        "                                      mean_pred[mask > 0].detach().cpu().numpy().flatten())\n",
        "\n",
        "                    for metric_name, metric in zip(metric_names, metrics):\n",
        "                      metric_dict[flow_setting][od_setting][metric_name] += metric\n",
        "\n",
        "            \"\"\"overall performance\"\"\"\n",
        "            for flow_setting in flow_settings:\n",
        "              for od_setting in od_settings:\n",
        "                metric_dict[flow_setting][od_setting][\"RMSE\"] = np.sqrt(metric_dict[flow_setting][od_setting][\"SE\"] / (metric_dict[flow_setting][od_setting][\"n_sample\"]+1e-32))\n",
        "                metric_dict[flow_setting][od_setting][\"MAE\"] = metric_dict[flow_setting][od_setting][\"AE\"] / (metric_dict[flow_setting][od_setting][\"n_sample\"]+1e-32)\n",
        "                metric_dict[flow_setting][od_setting][\"CPC\"] = 2. * metric_dict[flow_setting][od_setting][\"CPC_up\"] / (metric_dict[flow_setting][od_setting][\"CPC_bottom\"]+1e-32)\n",
        "                metric_dict[flow_setting][od_setting][\"MPIW\"] = metric_dict[flow_setting][od_setting][\"MPIW\"] / (metric_dict[flow_setting][od_setting][\"n_sample\"]+1e-32)\n",
        "                metric_dict[flow_setting][od_setting][\"PICP\"] = metric_dict[flow_setting][od_setting][\"PICP\"] / (metric_dict[flow_setting][od_setting][\"n_sample\"]+1e-32)\n",
        "        print('test', datetime.datetime.now() - start_time)\n",
        "        return metric_dict\n",
        "    @staticmethod\n",
        "    def RMSE(y_pred:np.array, y_true:np.array):\n",
        "        return np.sqrt(np.mean(np.square(y_pred - y_true)))\n",
        "    @staticmethod\n",
        "    def batch_SE(y_pred:np.array, y_true:np.array):\n",
        "        return np.sum(np.square(y_pred - y_true))\n",
        "    @staticmethod\n",
        "    def MAE(y_pred:np.array, y_true:np.array):\n",
        "        return np.mean(np.abs(y_pred - y_true))\n",
        "    @staticmethod\n",
        "    def batch_AE(y_pred:np.array, y_true:np.array):\n",
        "        return np.sum(np.abs(y_pred - y_true))\n",
        "    @staticmethod\n",
        "    def MAPE(y_pred:np.array, y_true:np.array, epsilon=1e-0):   # zero division\n",
        "        return np.mean(np.abs(y_pred - y_true) / (y_true + epsilon))\n",
        "    @staticmethod\n",
        "    def batch_APE(y_pred:np.array, y_true:np.array, epsilon=1e-0):   # zero division\n",
        "        return np.sum(np.abs(y_pred - y_true) / (y_true + epsilon))\n",
        "    @staticmethod\n",
        "    def CPC(y_pred:np.array, y_true:np.array, numerator_only=False):\n",
        "        if numerator_only:\n",
        "            tot = 1.0\n",
        "        else:\n",
        "            tot = (np.sum(y_pred) + np.sum(y_true))\n",
        "        if tot > 0:\n",
        "            return 2.0 * np.sum(np.minimum(y_pred, y_true)) / tot\n",
        "        else:\n",
        "            return 0.0\n",
        "    @staticmethod\n",
        "    def batch_CPC(y_pred:np.array, y_true:np.array, numerator_only=False):\n",
        "        return np.sum(np.minimum(y_pred, y_true)), (np.sum(y_pred) + np.sum(y_true))\n",
        "        # if numerator_only:\n",
        "        #     tot = 1.0\n",
        "        # else:\n",
        "        #     tot = (np.sum(y_pred) + np.sum(y_true))\n",
        "        # if tot > 0:\n",
        "        #     return 2.0 * np.sum(np.minimum(y_pred, y_true)) / tot\n",
        "        # else:\n",
        "        #     return 0.0\n",
        "    @staticmethod\n",
        "    def NRMSE(y_pred:np.array, y_true:np.array):\n",
        "        return np.sqrt(np.mean(np.square(y_pred - y_true))) / np.mean(y_true)\n",
        "    @staticmethod\n",
        "    def NMAE(y_pred:np.array, y_true:np.array):\n",
        "        return np.mean(np.abs(y_pred - y_true)) / np.mean(y_true)\n",
        "    @staticmethod\n",
        "    def PCC(y_pred:np.array, y_true:np.array):\n",
        "        return np.corrcoef(y_pred.flatten(), y_true.flatten())[0,1]\n",
        "    @staticmethod\n",
        "    def JSD(y_pred:np.array, y_true:np.array, mask: np.array):\n",
        "        \"\"\"y_pred: [..., n_des_station]\"\"\"\n",
        "        \"\"\"y_true: [..., n_des_station]\"\"\"\n",
        "        y_pred = y_pred.reshape(-1, y_pred.shape[-1])\n",
        "        y_true = y_true.reshape(-1, y_true.shape[-1])\n",
        "        mask = mask.reshape(-1, mask.shape[-1])\n",
        "        js_div_ls = []\n",
        "        for i in range(y_pred.shape[0]):\n",
        "            js_div = jensenshannon(y_pred[i][mask[i] > 0], y_true[i][mask[i] > 0])\n",
        "            js_div_ls.append(js_div)\n",
        "        return np.mean(np.array(js_div_ls))\n",
        "\n",
        "    @staticmethod\n",
        "    def batch_JSD(y_pred:np.array, y_true:np.array, mask: np.array):\n",
        "        \"\"\"y_pred: [..., n_des_station]\"\"\"\n",
        "        \"\"\"y_true: [..., n_des_station]\"\"\"\n",
        "        y_pred = y_pred.reshape(-1, y_pred.shape[-1])\n",
        "        y_true = y_true.reshape(-1, y_true.shape[-1])\n",
        "        mask = mask.reshape(-1, mask.shape[-1])\n",
        "        js_div_ls = []\n",
        "        for i in range(y_pred.shape[0]):\n",
        "            js_div = jensenshannon(y_pred[i][mask[i] > 0], y_true[i][mask[i] > 0])\n",
        "            js_div_ls.append(js_div)\n",
        "        return np.sum(np.array(js_div_ls))\n",
        "\n",
        "    def batch_zinb_MPIW(self, ns, ps, pis, targets, lower_percentile=0.1, upper_percentile=0.9):\n",
        "        if len(ns) == 0: return 0,0\n",
        "        max_range = self.data_class.max_od_cnt if self.setting == \"daily_cnt\" else self.data_class.max_od_density\n",
        "        x_values = np.arange(0, max_range.round())[:, None]  # Adjust according to the expected range of outcomes\n",
        "        pmf_nb = nbinom.pmf(x_values, ns, ps) # 50, 21854\n",
        "        pmf_zinb = pmf_nb.copy()\n",
        "        pmf_zinb = pmf_zinb * (1 - pis[None, :])\n",
        "        pmf_zinb[0] = pis + pmf_zinb[0]\n",
        "        cdf_zinb = np.cumsum(pmf_zinb, 0)\n",
        "        lower_bound = np.sum(cdf_zinb < lower_percentile, 0)\n",
        "        upper_bound = np.sum(cdf_zinb < upper_percentile, 0)\n",
        "        mpiw = upper_bound - lower_bound\n",
        "        picp = (targets >= lower_bound) * (targets <= upper_bound)\n",
        "        return np.sum(mpiw), np.sum(picp)\n",
        "\n",
        "    def batch_nb_MPIW(self, ns, ps, targets, lower_percentile=0.1, upper_percentile=0.9):\n",
        "        if len(ns) == 0: return 0,0\n",
        "        max_range = self.data_class.max_od_cnt if self.setting == \"daily_cnt\" else self.data_class.max_od_density\n",
        "        x_values = np.arange(0, max_range.round())[:, None]  # Adjust according to the expected range of outcomes\n",
        "        lower_bound, upper_bound = nbinom.interval(upper_percentile-lower_percentile, ns, ps)\n",
        "        mpiw = upper_bound - lower_bound\n",
        "        picp = (targets >= lower_bound) * (targets <= upper_bound)\n",
        "        return np.sum(mpiw), np.sum(picp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf2YhcGAM5ie"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jKTZsIrmoDNg"
      },
      "outputs": [],
      "source": [
        "\"\"\"load demand data\"\"\"\n",
        "local2global_station_dir = \"../data/feature/local2global_station.pkl\"\n",
        "month_od_opendays_dir = \"../data/feature/od_month_opendays.pkl\"\n",
        "month_station_opendays_dir = \"../data/feature/station_month_opendays.pkl\"\n",
        "month_od_nonzero_dir = \"../data/feature/od_month_nonzero.pkl\"\n",
        "feat_dir = \"../data/feature/BSS_feat_summary.csv\"\n",
        "od_feat_dir = \"../data/feature/od_interact_feat.npy\"\n",
        "month_feat_dir = \"../data/feature/BSS_month_feat_summary.pkl\"\n",
        "month_od_feat_dir = \"../data/feature/od_month_interact_feat.pkl\"\n",
        "manhattan_dir = \"../data/feature/od_manhattan.npy\"\n",
        "dist_knn_dir = \"../data/adj/month_geo_knn_0m.npy\"\n",
        "dist_adj_dir = \"../data/adj/Bike_geo_none.npy\"\n",
        "builtEnv_knn_dir = \"../data/adj/month_builtEnv_knn_0m.npy\"\n",
        "builtEnv_adj_dir = \"../data/adj/Bike_builtEnv_month.npy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zCgNFf6uGasE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "def setup_seed(seed):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvoirLq9UmHj"
      },
      "outputs": [],
      "source": [
        "setting = \"daily_density\"\n",
        "dist = \"zinb\"\n",
        "od_graph = True\n",
        "node_graph = False\n",
        "graph_conv_type = \"gat\"\n",
        "builtEnv = False\n",
        "learn_rate, weight_decay = 2e-3, 1e-6\n",
        "dropout = 0.5\n",
        "n_time = 8\n",
        "n_feat = 44\n",
        "n_interact = 13\n",
        "batch_size = 32\n",
        "optimizer = optim.Adam\n",
        "k=9\n",
        "data_input = DataInput(local2global_station_dir=local2global_station_dir,\n",
        "            month_od_opendays_dir=month_od_opendays_dir,\n",
        "            month_od_nonzero_dir=month_od_nonzero_dir,\n",
        "            feat_dir=feat_dir,\n",
        "            od_feat_dir=od_feat_dir,\n",
        "            month_feat_dir=month_feat_dir,\n",
        "            month_od_feat_dir=month_od_feat_dir,\n",
        "            manhattan_dir=manhattan_dir,\n",
        "            month_station_opendays_dir=month_station_opendays_dir,\n",
        "            dist_knn_dir=dist_knn_dir,\n",
        "            dist_adj_dir=dist_adj_dir,\n",
        "            builtEnv_knn_dir=builtEnv_knn_dir,\n",
        "            builtEnv_adj_dir=builtEnv_adj_dir,\n",
        "            n_train_timestep=50,\n",
        "            k=k,\n",
        "            setting=setting)\n",
        "data_generator = DataGenerator(data_input)\n",
        "data_processor = data_generator.get_data_loader(batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Test Model"
      ],
      "metadata": {
        "id": "bR7tsag0n8QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_dict = dict()\n",
        "for ep in range(5):\n",
        "    setup_seed(ep)\n",
        "    hour_prob_model_name = f\"zinb_gnn_k{k}_ep{ep}\"\n",
        "    hour_prob_model = Graph_NBNorm_ZeroInflated(dim_station=n_feat, dim_interact=n_interact, dropout=dropout, setting=setting,\n",
        "                          dist=dist, od_graph=od_graph, node_graph=node_graph,\n",
        "                          graph_conv_type=graph_conv_type, builtEnv=builtEnv).to(device)\n",
        "    trainer = ModelTrainer(hour_prob_model_name=hour_prob_model_name, hour_prob_model=hour_prob_model, hour_prob_epochs=5,\n",
        "                optimizer=optimizer, lr=learn_rate, wd=weight_decay,\n",
        "                dataset=data_input.dataset, data_class=data_input)\n",
        "    model_dir = \"../model\"\n",
        "    st_time = time.time()\n",
        "    trainer.train_hour_od(data_processor=data_processor, modes=[\"train\", \"valid\"], model_dir=model_dir,\n",
        "                        n_sample=100, weight=0.)\n",
        "    result = trainer.test_hour_od(epoch=ep, data_processor=data_processor, modes=[\"test\"], model_dir=model_dir)"
      ],
      "metadata": {
        "id": "whv-0cLFn7qa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}